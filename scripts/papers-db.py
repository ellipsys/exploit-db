#!/usr/bin/env python
# -*- coding: utf-8 -*-
##This script send requests to exploit database and download all reports of security
##All files with extension pdf, docx and txt
##And save in directory files
import requests as re
from bs4 import BeautifulSoup

def download(file,description,ext):
    with open("files/"+description+ext, 'wb') as f:
        f.write(file.content)


url = "https://www.exploit-db.com/papers/?order_by=date_published&order=desc&pg="

print "[+] Conectando con "+url
headers = {'User-Agent': 'Mozilla/5.0'}
i = 1
var = 0
ext = ""
for i in range(1,50):
    i = i + 1

    p = url+str(i)
    r = re.get(p, timeout=None, verify=False, headers=headers)
    s = BeautifulSoup(r.content, "html.parser")
    for tabla in s.find_all("tbody"):
        
        for tr in tabla.find_all("tr"):
            for description in tr.find_all("td", {"class":"description"}):
                var = var + 1
                
                for dlink in tr.find_all("td",{"class":"dlink"}):
                    for link in dlink.find_all("a", href=True):
                        url = link['href']
                        if url[-4:]==".pdf":
                            ext = ".pdf"
                        elif url[-5:] ==".docx":
                            ext = ".docx"
                        else:
                            ext = ".txt"
                            
                        for date in tr.find_all("td",{"class":"date"}):
                            for language in tr.find_all("td",{"class":"language"}):
                                for author in tr.find_all("td",{"class":"author"}):
                                    
                                    print "Name:\t"+description.text.strip()
                                    print "Autor:\t"+author.text.strip()
                                    print "Lang:\t"+language.text.strip()
                                    print "Date:\t"+date.text
                                    print "Links:\t"+url
                                    file = re.get(url, headers=headers, verify=False, timeout=None)
                                    download(file,description.text.strip().replace("/","-").replace(".","-"),ext)
                                    print "////////////////////////////////////////////////////"
                
        
                
                
print "Finish download %s links of papers "%var
